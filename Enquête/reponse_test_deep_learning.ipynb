{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réponses au test de deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce document je me suis plus attardé sur la éthode de résolution du problème en présentant les grandes étapes dans la démarche de résolution de chaque problème posé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Description de la mise en place d'un pipeline de traitement du process de détection d'un langage abusif sur le social web : \n",
    "Pour mettre sur pied ce pipeline il est question d’extraire un réseau conversationnel à partir de l’extraction des conversations récupérées, de ressortir les features et de les entrainer dans un classificateur. La définition des features peut imposer une analyse exploratoire en tenant compte du jargon associé  la population et au contexte. Les données recoltées peuvent être de sources diverses dans les lignes qui suivnet je vais présenter grossirement la méthode de récupération des messages contenus dans les tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des données de tweet \n",
    "\n",
    "# authentification à l'api \n",
    "# modules nécessaires\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas \n",
    "import re\n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuthHandler(\"clé\", \"cle_secret\")\n",
    "auth.set_access_token(\"token_d'acces\", \"token_secret\")\n",
    "api = tweepy.API(auth)\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Erreur d'authentification\")\n",
    "\n",
    "with open('tendance.csv','wb') as file :\n",
    "    colonne = ['user_name','tweet']\n",
    "    writer = csv.DictWriter(file, fieldnames=colonne)\n",
    "# Par exemple nous allons récupérer les derniers tweets en francais comprenant le mot greve\n",
    "    for tweet in api.search(q=\"greve\", lang=\"fr\", rpp=100000000):\n",
    "        writer.writerow('user_name': tweet.user.name,'tweet':tweet.text)\n",
    "\n",
    "# Nettoyage des données \n",
    "tendance  = pd.read_csv('tendance.csv', delimiter=';')\n",
    "\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    return df\n",
    "\n",
    "tendance = clean_text(tendance,'tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce niveau afin de plus se concentrer sur la méthode nous allons supposer qu'un traitement manuel des données récoltées nous a permis d'avoir une case décision pour marqué si le tweet est considéré comme abusif ou pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "pipeline_sgd = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',  TfidfTransformer()),\n",
    "    ('nb', SGDClassifier()),\n",
    "])\n",
    "model = pipeline_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose que nous disposons d'un modèle obtenu sur la base du pipeline précédent. Nous alons juste prédire la classe de chaque tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Utilisation d'un algorithme Hybrid CNN-LSTM pour la détection d'un langage abusif sur le social web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfacebook \n",
    "import pymongo\n",
    "\n",
    "api = pyfacebook.InstagramApi(app_id='your app id', app_secret='your app secret',short_token='your short token')\n",
    "content = api.get_medias(username='jacques_chirac')\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb = myclient[\"mydatabase\"]\n",
    "mycol = mydb[\"data\"]\n",
    "x = mycol.insert_one(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# execution \n",
    "result = model.predict(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
